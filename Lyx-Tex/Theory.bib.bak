% Encoding: UTF-8

@Unknown{unknown,
  author   = {Philipsen, Kristian},
  title    = {Theory Building: Using Abductive Search Strategies},
  month    = {01},
  year     = {2018},
  abstract = {Understanding how theory is built or comes into existence is a crucial part of knowledge production for all researchers. Knowledge production can be separated into context of discovery (problem/domain definition and initial ideas) and context of justification (hypothesis testing and hypothesis improvement). The context of justification is much better understood by researchers than is the context of discovery. The concepts of induction and deduction are used in the context of justification but they cannot explain how new ideas are created. This chapter proposes the concept of abduction to address this. Abduction can be conceptualized as making guesses. In a theory building process there is a need to make guesses when a researcher makes observations which are surprising in that they depart from existing theory. Other researchers have used abduction to describe and understand theory building. This paper identifies anomalies in the existing frameworks and proposes further development of the framework of systematic combining of the theory and empirical world. This chapter argues that the concept of abduction can contribute to understand how theory is created in theory testing, theory development and theory creation, but focuses on theory building in both realist and an interpretive research. 

Theory Building: Using Abductive Search Strategies. Available from: https://www.researchgate.net/publication/320085500_Theory_Building_Using_Abductive_Search_Strategies [accessed Nov 30 2017].},
  isbn     = {978-981-10-5006-0},
  pages    = {45-71},
}

@Article{doi:10.1086/288135,
  author   = {Paul E. Meehl},
  title    = {Theory-Testing in Psychology and Physics: A Methodological Paradox},
  journal  = {Philosophy of Science},
  year     = {1967},
  volume   = {34},
  number   = {2},
  pages    = {103-115},
  abstract = { Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by "success" is very weak, and becomes weaker with increased precision. "Statistical significance" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental "cuteness" and a free reliance upon ad hoc explanations to avoid refutation. },
  doi      = {10.1086/288135},
  eprint   = {https://doi.org/10.1086/288135},
  url      = { 
        https://doi.org/10.1086/288135
    
},
}

@Article{RSSA:RSSA12221,
  author  = {Hewson, Paul},
  title   = {Statistical Rethinking: a Bayesian Course with Examples in R and Stan R. McElreath, 2015 Boca Raton Chapman and Hall–CRC 470 pp., £60.99 ISBN 978-1-482-25344-3},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  year    = {2016},
  volume  = {179},
  number  = {4},
  pages   = {1131--1132},
  issn    = {1467-985X},
  doi     = {10.1111/rssa.12221},
  url     = {http://dx.doi.org/10.1111/rssa.12221},
}

@Article{doi:10.3102/1076998616659752,
  author  = {Tracy M. Sweet},
  title   = {A Review of Statistical Rethinking: A Bayesian Course With Examples in R and Stan},
  journal = {Journal of Educational and Behavioral Statistics},
  year    = {2017},
  volume  = {42},
  number  = {1},
  pages   = {107-110},
  doi     = {10.3102/1076998616659752},
  eprint  = {https://doi.org/10.3102/1076998616659752},
  url     = { 
        https://doi.org/10.3102/1076998616659752
    
},
}

@Article{doi:10.1177/0146167217729162,
  author   = {Andrew Gelman},
  title    = {The Failure of Null Hypothesis Significance Testing When Studying Incremental Changes, and What to Do About It},
  journal  = {Personality and Social Psychology Bulletin},
  year     = {2017},
  volume   = {0},
  number   = {0},
  pages    = {0146167217729162},
  note     = {PMID: 28914154},
  abstract = { A standard mode of inference in social and behavioral science is to establish stylized facts using statistical significance in quantitative studies. However, in a world in which measurements are noisy and effects are small, this will not work: selection on statistical significance leads to effect sizes which are overestimated and often in the wrong direction. After a brief discussion of two examples, one in economics and one in social psychology, we consider the procedural solution of open postpublication review, the design solution of devoting more effort to accurate measurements and within-person comparisons, and the statistical analysis solution of multilevel modeling and reporting all results rather than selection on significance. We argue that the current replication crisis in science arises in part from the ill effects of null hypothesis significance testing being used to study small effects with noisy data. In such settings, apparent success comes easy but truly replicable results require a more serious connection between theory, measurement, and data. },
  doi      = {10.1177/0146167217729162},
  eprint   = {https://doi.org/10.1177/0146167217729162},
  url      = { 
        https://doi.org/10.1177/0146167217729162
    
},
}

@Article{open2015estimating,
  author    = {Open Science Collaboration and others},
  title     = {Estimating the reproducibility of psychological science},
  journal   = {Science},
  year      = {2015},
  volume    = {349},
  number    = {6251},
  pages     = {aac4716},
  abstract  = {INTRODUCTION
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALE
There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTS
We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P < .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSION
No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.

Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.},
  publisher = {American Association for the Advancement of Science},
}

@Article{simmons2011false,
  author    = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
  title     = {False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant},
  journal   = {Psychological science},
  year      = {2011},
  volume    = {22},
  number    = {11},
  pages     = {1359--1366},
  abstract  = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.

Keywords methodology, motivated reasoning, publication, disclosure},
  keywords  = {rank3},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
}

@Article{doi:10.1080/01621459.2017.1289846,
  author    = {Blakeley B. McShane and David Gal},
  title     = {Statistical Significance and the Dichotomization of Evidence},
  journal   = {Journal of the American Statistical Association},
  year      = {2017},
  volume    = {112},
  number    = {519},
  pages     = {885-895},
  month     = {jul},
  abstract  = { ABSTRACTIn light of recent concerns about reproducibility and replicability, the ASA issued a Statement on Statistical Significance and p-values aimed at those who are not primarily statisticians. While the ASA Statement notes that statistical significance and p-values are “commonly misused and misinterpreted,” it does not discuss and document broader implications of these errors for the interpretation of evidence. In this article, we review research on how applied researchers who are not primarily statisticians misuse and misinterpret p-values in practice and how this can lead to errors in the interpretation of evidence. We also present new data showing, perhaps surprisingly, that researchers who are primarily statisticians are also prone to misuse and misinterpret p-values thus resulting in similar errors. In particular, we show that statisticians tend to interpret evidence dichotomously based on whether or not a p-value crosses the conventional 0.05 threshold for statistical significance. We discuss implications and offer recommendations. },
  doi       = {10.1080/01621459.2017.1289846},
  eprint    = {https://doi.org/10.1080/01621459.2017.1289846},
  publisher = {Taylor \& Francis},
  url       = { 
        https://doi.org/10.1080/01621459.2017.1289846
    
},
}

@InCollection{Philipsen2017,
  author    = {Kristian Philipsen},
  title     = {Theory Building: Using Abductive Search Strategies},
  booktitle = {Collaborative Research Design},
  publisher = {Springer Singapore},
  year      = {2017},
  pages     = {45--71},
  month     = {sep},
  abstract  = {Understanding how theory is built or comes into existence is a crucial part of knowledge production for all researchers. Knowledge production can be separated into context of discovery (problem/domain definition and initial ideas) and context of justification (hypothesis testing and hypothesis improvement). The context of justification is much better understood by researchers than is the context of discovery. The concepts of induction and deduction are used in the context of justification but they cannot explain how new ideas are created. This chapter proposes the concept of abduction to address this. Abduction can be conceptualized as making guesses. In a theory building process there is a need to make guesses when a researcher makes observations which are surprising in that they depart from existing theory. Other researchers have used abduction to describe and understand theory building. This paper identifies anomalies in the existing frameworks and proposes further development of the framework of systematic combining of the theory and empirical world. This chapter argues that the concept of abduction can contribute to understand how theory is created in theory testing, theory development and theory creation, but focuses on theory building in both realist and an interpretive research.},
  doi       = {10.1007/978-981-10-5008-4_3},
}

@Article{Gelman2014,
  author    = {Andrew Gelman and John Carlin},
  title     = {Beyond Power Calculations},
  journal   = {Perspectives on Psychological Science},
  year      = {2014},
  volume    = {9},
  number    = {6},
  pages     = {641--651},
  month     = {nov},
  abstract  = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  doi       = {10.1177/1745691614551642},
  publisher = {{SAGE} Publications},
}

@Article{Wicherts2017,
  author    = {Jelte Wicherts},
  title     = {The Weak Spots in Contemporary Science (and How to Fix Them)},
  journal   = {Animals},
  year      = {2017},
  volume    = {7},
  number    = {12},
  pages     = {90},
  month     = {nov},
  abstract  = {Simple Summary: Several fraud cases, widespread failure to replicate or reproduce seminal findings, and pervasive error in the scientific literature have led to a crisis of confidence in the biomedical, behavioral, and social sciences. In this review, the author discusses some of the core findings that point at weak spots in contemporary science and considers the human factors that underlie them. He delves into the human tendencies that create errors and biases in data collection, analyses, and reporting of research results. He presents several solutions to deal with observer bias, publication bias, the researcher’s tendency to exploit degrees of freedom in their analysis of data, low statistical power, and errors in the reporting of results, with a focus on the specific challenges in animal welfare research.
Abstract: In this review, the author discusses several of the weak spots in contemporary science, including scientific misconduct, the problems of post hoc hypothesizing (HARKing), outcome switching, theoretical bloopers in formulating research questions and hypotheses, selective reading of the literature, selective citing of previous results, improper blinding and other design failures, p-hacking or researchers’ tendency to analyze data in many different ways to find positive (typically significant) results, errors and biases in the reporting of results, and publication bias. The author presents some empirical results highlighting problems that lower the trustworthiness of reported results in scientific literatures, including that of animal welfare studies. Some of the underlying causes of these biases are discussed based on the notion that researchers are only human and hence are not immune to confirmation bias, hindsight bias, and minor ethical transgressions. The author discusses solutions in the form of enhanced transparency, sharing of data and materials, (post-publication) peer review, pre-registration, registered reports, improved training, reporting guidelines, replication, dealing with publication bias, alternative inferential techniques, power, and other statistical tools.
Keywords: reproducibility; replicability; validity; questionable research practices; meta-research},
  doi       = {10.3390/ani7120090},
  file      = {:C\:/Users/cjdua/Downloads/The Weak Spots in Contemporary Science (and How to Fix Them).pdf:PDF},
  publisher = {{MDPI} {AG}},
}

@Article{Carpenter2017,
  author    = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  title     = {Stan: A Probabilistic Programming Language},
  journal   = {Journal of Statistical Software},
  year      = {2017},
  volume    = {76},
  number    = {1},
  doi       = {10.18637/jss.v076.i01},
  publisher = {Foundation for Open Access Statistic},
}

@Article{Song2012,
  author    = {Xin-Yuan Song and Sik-Yum Lee},
  title     = {A tutorial on the Bayesian approach for analyzing structural equation models},
  journal   = {Journal of Mathematical Psychology},
  year      = {2012},
  volume    = {56},
  number    = {3},
  pages     = {135--148},
  month     = {jun},
  abstract  = {In this paper, we provide a tutorial exposition on the Bayesian approach in analyzing structural equation models (SEMs). SEMs, which can be regarded as regression models with observed and latent variables, have been widely applied to substantive research. However, the classical methods and most commercial software in this area are based on the covariance structure approach, which would encounter serious difficulties when dealing with complicated models and/or data structures. In contrast, the Bayesian approach has much more flexibility in handling complex situations. We give a brief introduction to SEMs and a detailed description of how to apply the Bayesian approach to this kind of model. Advantages of the Bayesian approach are discussed, and results obtained from a simulation study are provided for illustration. The intended audience is statisticians/methodologists who either know about SEMs or simple Bayesian statistics, and Ph.D. students in statistics, psychometrics, or mathematical psychology.},
  doi       = {10.1016/j.jmp.2012.02.001},
  file      = {:D\:/Github/Informs/Articles/Tutorial_Bayesian_SEM.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Bertsimas2015,
  author    = {Dimitris Bertsimas and Erik Brynjolfsson and Shachar Reichman and John Silberholz},
  title     = {{OR} Forum{\textemdash}Tenure Analytics: Models for Predicting Research Impact},
  journal   = {Operations Research},
  year      = {2015},
  volume    = {63},
  number    = {6},
  pages     = {1246--1261},
  month     = {dec},
  abstract  = {Tenure decisions, key decisions in academic institutions, are primarily based on subjective assessments of candidates. Using a large-scale bibliometric database containing 198,310 papers published 1975–2012 in the field of operations research (OR), we propose prediction models of whether a scholar would perform well on a number of future success metrics using statistical models trained with data from the scholar’s first five years of publication, a subset of the information available to tenure committees. These models, which use network centrality of the citation network, coauthorship network, and a dual network combining the two, significantly outperform simple predictive models based on citation counts alone. Using a data set of the 54 scholars who obtained a Ph.D. after 1995 and held an assistant professorship at a top-10 OR program in 2003 or earlier, these statistical models, using data up to five years after the scholar became an assistant professor and constrained to tenure the same number of candidates as tenure committees did, made a different decision than the tenure committees for 16 (30%) of the candidates. This resulted in a set of scholars with significantly better future A-journal paper counts, citation counts, and h-indexes than the scholars actually selected by tenure committees. These results show that analytics can complement the tenure decision-making process in academia and improve the prediction of academic impact.},
  doi       = {10.1287/opre.2015.1447},
  publisher = {Institute for Operations Research and the Management Sciences ({INFORMS})},
}

@Article{Benjamin2018,
  author    = {Benjamin, Daniel J and Berger, James O and Johannesson, Magnus and Nosek, Brian A and Wagenmakers, E-J and Berk, Richard and Bollen, Kenneth A and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and others},
  title     = {Redefine statistical significance},
  journal   = {Nature Human Behaviour},
  year      = {2018},
  volume    = {2},
  number    = {1},
  pages     = {6},
  doi       = {10.3390/ani7120090},
  file      = {:D\:/Dropbox/Bayes_Reading/Redefine Statistical Significance.pdf:PDF},
  keywords  = {reproducibility, replicability, validity, questionable research practices, meta-research},
  publisher = {Nature Publishing Group},
  url       = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5742784/},
}

@Article{Lakens2017,
  author  = {Lakens, D and Adolfi, FG and Albers, CJ and Anvari, F and Apps, MAJ and Argamon, SE and Zwaan, RA},
  title   = {Justify your alpha: A response to “Redefine statistical significance”},
  journal = {Retrieved from psyarxiv. com/9s3y6},
  year    = {2017},
  file    = {:D\:/Dropbox/Bayes_Reading/Justify Your Alpha.pdf:PDF},
}

@Article{Mayo1970,
  author  = {Mayo, Palindromes By},
  title   = {A. Birnbaum: Statistical Methods in Scientific Inference},
  journal = {Nature},
  year    = {1970},
  volume  = {225},
  pages   = {1033},
}

@Article{doi:10.1177/0149206316647099,
  author   = {Jeremy D. Meuser and William L. Gardner and Jessica E. Dinh and Jinyu Hu and Robert C. Liden and Robert G. Lord},
  title    = {A Network Analysis of Leadership Theory: The Infancy of Integration},
  journal  = {Journal of Management},
  year     = {2016},
  volume   = {42},
  number   = {5},
  pages    = {1374-1403},
  abstract = { We investigated the status of leadership theory integration by reviewing 14 years of published research (2000 through 2013) in 10 top journals (864 articles). The authors of these articles examined 49 leadership approaches/theories, and in 293 articles, 3 or more of these leadership approaches were included in their investigations. Focusing on these articles that reflected relatively extensive integration, we applied an inductive approach and used graphic network analysis as a guide for drawing conclusions about the status of leadership theory integration. All 293 articles included in the analysis identified 1 focal theory that was integrated with 2 or more supporting leadership theories. The 6 leadership approaches most often appearing as the focal theory were transformational leadership, charismatic leadership, strategic leadership, leadership and diversity, participative/shared leadership, and the trait approach to leadership. On the basis of inductive reflections on our analysis, we make two key observations. First, the 49 focal leadership theories qualify as middle-range theories that are ripe for integration. Second, drawing from social network theory, we introduce the term “theoretical neighborhood” to describe the focal theoretical networks. Our graphical inductive analyses reveal potential connections among neighboring middle-range leadership theories that merit investigation and, hence, identify promising future directions for achieving greater theoretical integration. We provide an online supplement with 10 additional leadership theory graphs and analyses: leadership in teams and decision groups, ethical leadership, leader and follower cognitions, leadership emergence, leadership development, emotions and leadership, implicit leadership, leader-member exchange, authentic leadership, and identity and identification process theories of leadership. },
  doi      = {10.1177/0149206316647099},
  eprint   = {https://doi.org/10.1177/0149206316647099},
  file     = {:D\:/Github/Informs/Articles/0149206316647099.pdf:PDF},
  url      = { 
        https://doi.org/10.1177/0149206316647099
    
},
}

@Article{doi:10.1177/0149206316675031,
  author   = {Roy Suddaby and William M. Foster},
  title    = {History and Organizational Change},
  journal  = {Journal of Management},
  year     = {2017},
  volume   = {43},
  number   = {1},
  pages    = {19-38},
  abstract = { This research commentary introduces historical consciousness to studying organizational change. Most theories of organizational change contain within them implicit assumptions about history. Made explicit, these assumptions tend to cluster into different models of change that vary by the assumed objectivity of the past and the associated malleability of the future. We explore and elaborate the implicit assumptions of history. We identify four implicit models of history in the change literature: History-as-Fact, History-as-Power, History-as-Sensemaking, and History-as-Rhetoric. We discuss the implications of theorizing organizational change from each of these views of history and outline future directions for studying change with a heightened understanding of history. },
  doi      = {10.1177/0149206316675031},
  eprint   = {https://doi.org/10.1177/0149206316675031},
  file     = {:D\:/Github/Informs/Articles/0149206316675031.pdf:PDF},
  url      = { 
        https://doi.org/10.1177/0149206316675031
    
},
}

@Article{doi:10.1177/0149206313503018,
  author   = {Philip L. Roth and Philip Bobko and Chad H. Van Iddekinge and Jason B. Thatcher},
  title    = {Social Media in Employee-Selection-Related Decisions: A Research Agenda for Uncharted Territory},
  journal  = {Journal of Management},
  year     = {2016},
  volume   = {42},
  number   = {1},
  pages    = {269-298},
  abstract = { Social media (SM) pervades our society. One rapidly growing application of SM is its use in personnel decision making. Organizations are increasingly searching SM (e.g., Facebook) to gather information about potential employees. In this article, we suggest that organizational practice has outpaced the scientific study of SM assessments in an area that has important consequences for individuals (e.g., being selected for work), organizations (e.g., successfully predicting job performance or withdrawal), and society (e.g., consequent adverse impact/diversity). We draw on theory and research from various literatures to advance a research agenda that addresses this gap between practice and research. Overall, we believe this is a somewhat rare moment in the human resources literature when a new class of selection methods arrives on the scene, and we urge researchers to help understand the implications of using SM assessments for personnel decisions. },
  doi      = {10.1177/0149206313503018},
  eprint   = {https://doi.org/10.1177/0149206313503018},
  file     = {:D\:/Github/Informs/Articles/0149206313503018.pdf:PDF},
  url      = { 
        https://doi.org/10.1177/0149206313503018
    
},
}

@Article{doi:10.1177/0001839216674457,
  author   = {Kevin Zheng Zhou and Gerald Yong Gao and Hongxin Zhao},
  title    = {State Ownership and Firm Innovation in China: An Integrated View of Institutional and Efficiency Logics},
  journal  = {Administrative Science Quarterly},
  year     = {2017},
  volume   = {62},
  number   = {2},
  pages    = {375-404},
  abstract = { Using two longitudinal panel datasets of Chinese manufacturing firms, we assess whether state ownership benefits or impedes firms’ innovation. We show that state ownership in an emerging economy enables a firm to obtain crucial R\&D resources but makes the firm less efficient in using those resources to generate innovation, and we find that a minority state ownership is an optimal structure for innovation development in this context. Moreover, the inefficiency of state ownership in transforming R\&D input into innovation output decreases when industrial competition is high, as well as for start-up firms. Our findings integrate the efficiency logic (agency theory), which views state ownership as detrimental to innovation, and institutional logic, which notes that governments in emerging economies have critical influences on regulatory policies and control over scarce resources. We discuss the implications of these findings for research on state ownership and firm innovation in emerging economies. },
  doi      = {10.1177/0001839216674457},
  eprint   = {https://doi.org/10.1177/0001839216674457},
  file     = {:D\:/Github/Informs/Articles/0001839216674457.pdf:PDF},
  url      = { 
        https://doi.org/10.1177/0001839216674457
    
},
}

@Article{doi:10.1177/0001839217705375,
  author   = {Letian Zhang},
  title    = {A Fair Game? Racial Bias and Repeated Interaction between NBA Coaches and Players},
  journal  = {Administrative Science Quarterly},
  year     = {2017},
  volume   = {62},
  number   = {4},
  pages    = {603-625},
  abstract = { There is strong evidence of racial bias in organizations but little understanding of how it changes with repeated interaction. This study proposes that repeated interaction has the potential to reduce racial bias, but its moderating effects may be limited to the treatment of individuals rather than of entire racial groups. Using data from 2,360 National Basketball Association (NBA) players and 163 coaches from 1955 to 2000, I find that players receive more playing time under coaches of the same race, even though there is no difference in their performance. This racial bias is greatly reduced, however, as the player and the coach spend more time on the same team, suggesting that repeated interaction minimizes coaches’ biases toward their players. But it does not reduce coaches’ racial biases in general. Even after years of coaching other-race players, coaches still exhibit the same levels of racial bias as they did upon first entering the league. These results suggest that repeated workplace interaction is effective in reducing racial bias toward individuals but not toward groups, making an important contribution to the literature on organizational inequality. },
  doi      = {10.1177/0001839217705375},
  eprint   = {https://doi.org/10.1177/0001839217705375},
  file     = {:D\:/Github/Informs/Articles/0001839217705375.pdf:PDF},
  url      = { 
        https://doi.org/10.1177/0001839217705375
    
},
}

@Article{Gelman2014a,
  author        = {Gelman, Andrew and Carlin, John},
  title         = {Beyond power calculations: assessing type S (sign) and type M (magnitude) errors},
  journal       = {Perspectives on Psychological Science},
  year          = {2014},
  volume        = {9},
  number        = {6},
  pages         = {641--651},
  __markedentry = {[cjdua:4]},
  file          = {:D\:/Github/Informs/Articles/1745691614551642.pdf:PDF},
  publisher     = {Sage Publications Sage CA: Los Angeles, CA},
}

@Article{Sonenshein2017,
  author    = {Scott Sonenshein and Kristen Nault and Otilia Obodaru},
  title     = {Competition of a Different Flavor: How a Strategic Group Identity Shapes Competition and Cooperation},
  journal   = {Administrative Science Quarterly},
  year      = {2017},
  volume    = {62},
  number    = {4},
  pages     = {626--656},
  month     = {apr},
  abstract  = {Using an inductive study of 41 gourmet food trucks, we develop theory about how firms form a strategic group identity that shapes both competitive and cooperative behaviors among its members. Based on an analysis of group prototypes, we find that members cooperate to help each other meet the central tendencies of the group—the properties that typical group members have—and yet compete to strive for the ideal tendencies of the group—the attributes of members held in highest regard. These competitive and cooperative dynamics lead to three surprising consequences in light of previous research on strategic groups: (1) existing members of the strategic group help new firms enter the market; (2) resource scarcity leads to cooperation, not competition; and (3) when competition does emerge, it focuses on status within the group and not on price. To make sense of these empirical puzzles, we develop theory around the micro identity processes that allow a strategic group’s identity to persist and to shape its member firms’ behaviors, which alters how scholars understand the inner workings of strategic groups and their impact on both firms and markets.},
  doi       = {10.1177/0001839217704849},
  file      = {:D\:/Github/Informs/Articles/0001839217704849[1759].pdf:PDF},
  keywords  = {organizational identity, interorganizational relations, competition, prosocial behavior, gourmet food trucks},
  publisher = {{SAGE} Publications},
}

@Article{Simmons2011,
  author    = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
  title     = {False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant},
  journal   = {Psychological science},
  year      = {2011},
  volume    = {22},
  number    = {11},
  pages     = {1359--1366},
  abstract  = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  doi       = {10.1177/0956797611417632},
  file      = {:D\:/Github/Informs/Articles/0956797611417632.pdf:PDF},
  keywords  = {methodology, motivated reasoning, publication, disclosure},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
}

@Article{McShane2016,
  author    = {Blakeley B. McShane and David Gal},
  title     = {Blinding Us to the Obvious? The Effect of Statistical Training on the Evaluation of Evidence},
  journal   = {Management Science},
  year      = {2016},
  volume    = {62},
  number    = {6},
  pages     = {1707--1718},
  month     = {jun},
  abstract  = {Statistical training helps individuals analyze and interpret data. However, the emphasis placed on null hypothesis significance testing in academic training and reporting may lead researchers to interpret evidence dichotomously rather than continuously. Consequently, researchers may either disregard evidence that fails to attain statistical significance or undervalue it relative to evidence that attains statistical significance. Surveys of researchers across a wide variety of fields (including medicine, epidemiology, cognitive science, psychology, business, and economics) show that a substantial majority does indeed do so. This phenomenon is manifest both in researchers’ interpretations of descriptions of evidence and in their likelihood judgments. Dichotomization of evidence is reduced though still present when researchers are asked to make decisions based on the evidence, particularly when the decision outcome is personally consequential. Recommendations are offered.},
  doi       = {10.1287/mnsc.2015.2212},
  file      = {:D\:/Github/Informs/Articles/mgmtsci_pvalue.pdf:PDF},
  keywords  = {sociology of science; evaluation of evidence; strength of evidence; null hypothesis; significance testing; p-values; description; inference; judgment; choice},
  publisher = {Institute for Operations Research and the Management Sciences ({INFORMS})},
}

@Article{Nuzzo2014,
  author    = {Regina Nuzzo},
  title     = {Scientific method: Statistical errors},
  journal   = {Nature},
  year      = {2014},
  volume    = {506},
  number    = {7487},
  pages     = {150--152},
  month     = {feb},
  doi       = {10.1038/506150a},
  file      = {:D\:/Github/Informs/Articles/506150a (1).pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Vrieze2018,
  author    = {Jop Vrieze},
  title     = {Nearly 100 scientists spent 2 months on Google Docs to redefine the p-value. Here's what they came up with},
  journal   = {Science},
  year      = {2018},
  month     = {jan},
  doi       = {10.1126/science.aat0471},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Heiberger2016,
  author    = {Raphael H. Heiberger and Jan R. Riebling},
  title     = {Installing computational social science: Facing the challenges of new information and communication technologies in social science},
  journal   = {Methodological Innovations},
  year      = {2016},
  volume    = {9},
  pages     = {205979911562276},
  month     = {mar},
  abstract  = {Today’s world allows people to connect over larger distances and in shorter intervals than ever before, widely monitored by massive online data sources. Ongoing worldwide computerization has led to completely new opportunities for social scientists to conceive human interactions and relations in unknown precision and quantities. However, the large data sets require techniques that are more likely to be found in computer and natural sciences than in the established fields of social relations. In order to facilitate the participation of social scientists in an emerging interdisciplinary research branch of “computational social science,” we propose in this article the usage of the Python programming language. First, we carve out its capacity to handle “Big Data” in suitable formats. Second, we introduce programming libraries to analyze large networks and big text corpora, conduct simulations, and compare their performance to their counterparts in the R environment. Furthermore, we highlight practical tools implemented in Python for operational tasks like preparing presentations. Finally, we discuss how the process of writing code may help to exemplify theoretical concepts and could lead to empirical applications that gain a better understanding of the social processes initiated by the truly global connections of the Internet era.},
  doi       = {10.1177/2059799115622763},
  file      = {:D\:/Github/Informs/Articles/2059799115622763.pdf:PDF},
  publisher = {{SAGE} Publications},
}

@Article{2018arXiv180302436A,
  author        = {{Antonoyiannakis}, M.},
  title         = {{Impact Factors and the Central Limit Theorem}},
  journal       = {ArXiv e-prints},
  year          = {2018},
  month         = mar,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180302436A},
  archiveprefix = {arXiv},
  eprint        = {1803.02436},
  file          = {:D\:/Github/Informs/Articles/1803.02436.pdf:PDF},
  keywords      = {Physics - Physics and Society, Computer Science - Digital Libraries},
  primaryclass  = {physics.soc-ph},
}

@Article{doi:10.1093/ajcn/nqx067,
  author  = {Kroeger, Cynthia M and Garza, Cutberto and Lynch, Christopher J and Myers, Esther and Rowe, Sylvia and Schneeman, Barbara O and Sharma, Arya M and Allison, David B},
  title   = {Scientific rigor and credibility in the nutrition research landscape},
  journal = {The American Journal of Clinical Nutrition},
  year    = {2018},
  volume  = {107},
  number  = {3},
  pages   = {484-494},
  doi     = {10.1093/ajcn/nqx067},
  eprint  = {/oup/backfile/content_public/journal/ajcn/107/3/10.1093_ajcn_nqx067/2/nqx067.pdf},
  file    = {:D\:/Github/Informs/Articles/nqx067.pdf:PDF},
  url     = { + http://dx.doi.org/10.1093/ajcn/nqx067},
}

@Book{hey2009fourth,
  title     = {The fourth paradigm: data-intensive scientific discovery},
  publisher = {Microsoft research Redmond, WA},
  year      = {2009},
  author    = {Hey, Tony and Tansley, Stewart and Tolle, Kristin M and others},
  volume    = {1},
}

@Article{bell2009beyond,
  author    = {Bell, Gordon and Hey, Tony and Szalay, Alex},
  title     = {Beyond the data deluge},
  journal   = {Science},
  year      = {2009},
  volume    = {323},
  number    = {5919},
  pages     = {1297--1298},
  publisher = {American Association for the Advancement of Science},
}

@Comment{jabref-meta: databaseType:bibtex;}
