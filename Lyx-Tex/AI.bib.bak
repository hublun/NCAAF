% Encoding: UTF-8

@Article{2017arXiv171106402A,
  author        = {{Avati}, A. and {Jung}, K. and {Harman}, S. and {Downing}, L. and {Ng}, A. and {Shah}, N.~H.},
  title         = {{Improving Palliative Care with Deep Learning}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {Improving the quality of end-of-life care for hospitalized patients is a priority for healthcare organizations. Studies have shown that physicians tend to over-estimate prognoses, which in combination with treatment inertia results in a mismatch between patients wishes and actual care at the end of life. We describe a method to address this problem using Deep Learning and Electronic Health Record (EHR) data, which is currently being piloted, with Institutional Review Board approval, at an academic medical center. The EHR data of admitted patients are automatically evaluated by an algorithm, which brings patients who are likely to benefit from palliative care services to the attention of the Palliative Care team. The algorithm is a Deep Neural Network trained on the EHR data from previous years, to predict all-cause 3-12 month mortality of patients as a proxy for patients that could benefit from palliative care. Our predictions enable the Palliative Care team to take a proactive approach in reaching out to such patients, rather than relying on referrals from treating physicians, or conduct time consuming chart reviews of all patients. We also present a novel interpretation technique which we use to provide explanations of the model's predictions.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171106402A},
  archiveprefix = {arXiv},
  eprint        = {1711.06402},
  keywords      = {Computer Science - Computers and Society, Computer Science - Learning, Statistics - Machine Learning},
  primaryclass  = {cs.CY},
}

@Article{2017arXiv171105225R,
  author        = {{Rajpurkar}, P. and {Irvin}, J. and {Zhu}, K. and {Yang}, B. and {Mehta}, H. and {Duan}, T. and {Ding}, D. and {Bagul}, A. and {Langlotz}, C. and {Shpanskaya}, K. and {Lungren}, M.~P. and {Ng}, A.~Y.},
  title         = {{CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on pneumonia detection on both sensitivity and specificity. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171105225R},
  archiveprefix = {arXiv},
  eprint        = {1711.05225},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
  primaryclass  = {cs.CV},
}

@Article{2017arXiv171110337L,
  author        = {{Lucic}, M. and {Kurach}, K. and {Michalski}, M. and {Gelly}, S. and {Bousquet}, O.},
  title         = {{Are GANs Created Equal? A Large-Scale Study}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the original one.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171110337L},
  archiveprefix = {arXiv},
  eprint        = {1711.10337},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning},
  primaryclass  = {stat.ML},
}

@Article{Poplin2018,
  author    = {Ryan Poplin and Avinash V. Varadarajan and Katy Blumer and Yun Liu and Michael V. McConnell and Greg S. Corrado and Lily Peng and Dale R. Webster},
  title     = {Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning},
  journal   = {Nature Biomedical Engineering},
  year      = {2018},
  month     = {feb},
  abstract  = {Traditionally, medical discoveries are made by observing associations, making hypotheses from them and then designing and running experiments to test the hypotheses. However, with medical images, observing and quantifying associations can often be difficult because of the wide variety of features, patterns, colours, values and shapes that are present in real data. Here, we show that deep learning can extract new knowledge from retinal fundus images. Using deep-learning models trained on data from 284,335 patients and validated on two independent datasets of 12,026 and 999 patients, we predicted cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as age (mean absolute error within 3.26 years), gender (area under the receiver operating characteristic curve (AUC) = 0.97), smoking status (AUC = 0.71), systolic blood pressure (mean absolute error within 11.23 mmHg) and major adverse cardiac events (AUC = 0.70). We also show that the trained deep-learning models used anatomical features, such as the optic disc or blood vessels, to generate each prediction.

},
  doi       = {10.1038/s41551-018-0195-0},
  publisher = {Springer Nature},
}

@Comment{jabref-meta: databaseType:bibtex;}
