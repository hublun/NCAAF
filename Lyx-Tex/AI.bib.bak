% Encoding: UTF-8

@Article{2017arXiv171106402A,
  author        = {{Avati}, A. and {Jung}, K. and {Harman}, S. and {Downing}, L. and {Ng}, A. and {Shah}, N.~H.},
  title         = {{Improving Palliative Care with Deep Learning}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {Improving the quality of end-of-life care for hospitalized patients is a priority for healthcare organizations. Studies have shown that physicians tend to over-estimate prognoses, which in combination with treatment inertia results in a mismatch between patients wishes and actual care at the end of life. We describe a method to address this problem using Deep Learning and Electronic Health Record (EHR) data, which is currently being piloted, with Institutional Review Board approval, at an academic medical center. The EHR data of admitted patients are automatically evaluated by an algorithm, which brings patients who are likely to benefit from palliative care services to the attention of the Palliative Care team. The algorithm is a Deep Neural Network trained on the EHR data from previous years, to predict all-cause 3-12 month mortality of patients as a proxy for patients that could benefit from palliative care. Our predictions enable the Palliative Care team to take a proactive approach in reaching out to such patients, rather than relying on referrals from treating physicians, or conduct time consuming chart reviews of all patients. We also present a novel interpretation technique which we use to provide explanations of the model's predictions.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171106402A},
  archiveprefix = {arXiv},
  eprint        = {1711.06402},
  keywords      = {Computer Science - Computers and Society, Computer Science - Learning, Statistics - Machine Learning},
  primaryclass  = {cs.CY},
}

@Article{2017arXiv171105225R,
  author        = {{Rajpurkar}, P. and {Irvin}, J. and {Zhu}, K. and {Yang}, B. and {Mehta}, H. and {Duan}, T. and {Ding}, D. and {Bagul}, A. and {Langlotz}, C. and {Shpanskaya}, K. and {Lungren}, M.~P. and {Ng}, A.~Y.},
  title         = {{CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on pneumonia detection on both sensitivity and specificity. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171105225R},
  archiveprefix = {arXiv},
  eprint        = {1711.05225},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
  primaryclass  = {cs.CV},
}

@Article{2017arXiv171110337L,
  author        = {{Lucic}, M. and {Kurach}, K. and {Michalski}, M. and {Gelly}, S. and {Bousquet}, O.},
  title         = {{Are GANs Created Equal? A Large-Scale Study}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the original one.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171110337L},
  archiveprefix = {arXiv},
  eprint        = {1711.10337},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning},
  primaryclass  = {stat.ML},
}

@Article{Poplin2018,
  author    = {Ryan Poplin and Avinash V. Varadarajan and Katy Blumer and Yun Liu and Michael V. McConnell and Greg S. Corrado and Lily Peng and Dale R. Webster},
  title     = {Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning},
  journal   = {Nature Biomedical Engineering},
  year      = {2018},
  month     = {feb},
  abstract  = {Traditionally, medical discoveries are made by observing associations, making hypotheses from them and then designing and running experiments to test the hypotheses. However, with medical images, observing and quantifying associations can often be difficult because of the wide variety of features, patterns, colours, values and shapes that are present in real data. Here, we show that deep learning can extract new knowledge from retinal fundus images. Using deep-learning models trained on data from 284,335 patients and validated on two independent datasets of 12,026 and 999 patients, we predicted cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as age (mean absolute error within 3.26 years), gender (area under the receiver operating characteristic curve (AUC) = 0.97), smoking status (AUC = 0.71), systolic blood pressure (mean absolute error within 11.23 mmHg) and major adverse cardiac events (AUC = 0.70). We also show that the trained deep-learning models used anatomical features, such as the optic disc or blood vessels, to generate each prediction.

},
  doi       = {10.1038/s41551-018-0195-0},
  file      = {:D\:/Github/Informs/Articles/41551_2018_195_MOESM1_ESM.pdf:PDF;:D\:/Github/Informs/Articles/Prediction of cardiovascular risk factors from.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Brown2017,
  author    = {Noam Brown and Tuomas Sandholm},
  title     = {Superhuman {AI} for heads-up no-limit poker: Libratus beats top professionals},
  journal   = {Science},
  year      = {2017},
  volume    = {359},
  number    = {6374},
  pages     = {418--424},
  month     = {dec},
  doi       = {10.1126/science.aao1733},
  file      = {:D\:/Dropbox/AI Impact/science.aao1733.full.pdf:PDF},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{shallue2018identifying,
  author    = {Shallue, Christopher J and Vanderburg, Andrew},
  title     = {Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90},
  journal   = {The Astronomical Journal},
  year      = {2018},
  volume    = {155},
  number    = {2},
  pages     = {94},
  file      = {:C\:/Users/cjdua/Documents/kepler90i.pdf:PDF},
  publisher = {IOP Publishing},
}

@Article{Chen2017,
  author    = {Gang Chen and Yaqiong Xiao and Paul A Taylor and Justin K Rajendra and Tracy Riggins and Fengji Geng and Elizabeth Redcay and Robert W Cox},
  title     = {Handling Multiplicity in Neuroimaging through Bayesian Lenses with Multilevel Modeling},
  year      = {2017},
  month     = {dec},
  doi       = {10.1101/238998},
  file      = {:C\:/Users/cjdua/Documents/238998.full.pdf:PDF},
  publisher = {Cold Spring Harbor Laboratory},
}

@Article{marcus2018deep,
  author  = {Marcus, Gary},
  title   = {Deep Learning: A Critical Appraisal},
  journal = {arXiv preprint arXiv:1801.00631},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1801.00631.pdf:PDF},
}

@Article{Boullier2016,
  author  = {Boullier, Dominique},
  title   = {Big data challenges for the social sciences: From society and opinion to replications},
  journal = {arXiv preprint arXiv:1607.05034},
  year    = {2016},
  file    = {:C\:/Users/cjdua/Documents/EBul-Boullier-Jul2017.pdf:PDF},
}

@Article{Jia2017,
  author  = {Jia, Jie and Zhou, Honggang and Li, Yunchun},
  title   = {Using Deep Neural Network Approximate Bayesian Network},
  journal = {arXiv preprint arXiv:1801.00282},
  year    = {2017},
  file    = {:C\:/Users/cjdua/Documents/1801.00282.pdf:PDF},
}

@Article{pearl2018theoretical,
  author  = {Pearl, Judea},
  title   = {Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution},
  journal = {arXiv preprint arXiv:1801.04016},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1801.04016.pdf:PDF},
}

@InCollection{taddy2018technological,
  author    = {Taddy, Matt},
  title     = {The Technological Elements of Artificial Intelligence},
  booktitle = {Economics of Artificial Intelligence},
  publisher = {University of Chicago Press},
  year      = {2018},
  file      = {:C\:/Users/cjdua/Documents/c14021.pdf:PDF},
}

@Article{sargsyan2018embedded,
  author  = {Sargsyan, Khachik and Huan, Xun and Najm, Habib N},
  title   = {Embedded Model Error Representation for Bayesian Model Calibration},
  journal = {arXiv preprint arXiv:1801.06768},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1801.06768.pdf:PDF},
}

@Article{rajkomar2018scalable,
  author  = {Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M and Hajaj, Nissan and Liu, Peter J and Liu, Xiaobing and Sun, Mimi and Sundberg, Patrik and Yee, Hector and others},
  title   = {Scalable and accurate deep learning for electronic health records},
  journal = {arXiv preprint arXiv:1801.07860},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1801.07860.pdf:PDF},
}

@Article{williams2006gaussian,
  author  = {Williams, Christopher KI and Rasmussen, Carl Edward},
  title   = {Gaussian processes for machine learning},
  journal = {the MIT Press},
  year    = {2006},
  volume  = {2},
  number  = {3},
  pages   = {4},
  file    = {:C\:/Users/cjdua/Documents/RW.pdf:PDF},
}

@Article{Caliskan2017,
  author    = {Aylin Caliskan and Joanna J. Bryson and Arvind Narayanan},
  title     = {Semantics derived automatically from language corpora contain human-like biases},
  journal   = {Science},
  year      = {2017},
  volume    = {356},
  number    = {6334},
  pages     = {183--186},
  month     = {apr},
  abstract  = {Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicate a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
  doi       = {10.1126/science.aal4230},
  file      = {:C\:/Users/cjdua/Documents/155388415.pdf:PDF},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Theis2015,
  author      = {Lucas Theis and Aäron van den Oord and Matthias Bethge},
  title       = {A note on the evaluation of generative models},
  abstract    = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
  date        = {2015-11-05},
  eprint      = {1511.01844v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  keywords    = {stat.ML, cs.LG},
}

@Article{Lucic2017,
  author        = {Mario Lucic and Karol Kurach and Marcin Michalski and Sylvain Gelly and Olivier Bousquet},
  title         = {Are GANs Created Equal? A Large-Scale Study},
  __markedentry = {[cjdua:]},
  abstract      = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the original one.},
  date          = {2017-11-28},
  eprint        = {1711.10337v3},
  eprintclass   = {stat.ML},
  eprinttype    = {arXiv},
  file          = {online:http\://arxiv.org/pdf/1711.10337v3:PDF;:D\:/Github/Informs/Articles/2018.arXiv.1711.10337.pdf:PDF},
  keywords      = {stat.ML, cs.LG},
}

@Article{Koh2017,
  author        = {Pang Wei Koh and Percy Liang},
  title         = {Understanding Black-box Predictions via Influence Functions},
  __markedentry = {[cjdua:6]},
  abstract      = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  date          = {2017-03-14},
  eprint        = {1703.04730v2},
  eprintclass   = {stat.ML},
  eprinttype    = {arXiv},
  file          = {online:D\:/Github/Informs/Articles/2017.arXiv.1703.04730.pdf:URL},
  keywords      = {stat.ML, cs.AI, cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}
