% Encoding: UTF-8

@Unknown{unknown,
  author   = {Philipsen, Kristian},
  title    = {Theory Building: Using Abductive Search Strategies},
  month    = {01},
  year     = {2018},
  abstract = {Understanding how theory is built or comes into existence is a crucial part of knowledge production for all researchers. Knowledge production can be separated into context of discovery (problem/domain definition and initial ideas) and context of justification (hypothesis testing and hypothesis improvement). The context of justification is much better understood by researchers than is the context of discovery. The concepts of induction and deduction are used in the context of justification but they cannot explain how new ideas are created. This chapter proposes the concept of abduction to address this. Abduction can be conceptualized as making guesses. In a theory building process there is a need to make guesses when a researcher makes observations which are surprising in that they depart from existing theory. Other researchers have used abduction to describe and understand theory building. This paper identifies anomalies in the existing frameworks and proposes further development of the framework of systematic combining of the theory and empirical world. This chapter argues that the concept of abduction can contribute to understand how theory is created in theory testing, theory development and theory creation, but focuses on theory building in both realist and an interpretive research. 

Theory Building: Using Abductive Search Strategies. Available from: https://www.researchgate.net/publication/320085500_Theory_Building_Using_Abductive_Search_Strategies [accessed Nov 30 2017].},
  isbn     = {978-981-10-5006-0},
  pages    = {45-71},
}

@Article{doi:10.1086/288135,
  author   = {Paul E. Meehl},
  title    = {Theory-Testing in Psychology and Physics: A Methodological Paradox},
  journal  = {Philosophy of Science},
  year     = {1967},
  volume   = {34},
  number   = {2},
  pages    = {103-115},
  abstract = { Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by "success" is very weak, and becomes weaker with increased precision. "Statistical significance" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental "cuteness" and a free reliance upon ad hoc explanations to avoid refutation. },
  doi      = {10.1086/288135},
  eprint   = {https://doi.org/10.1086/288135},
  url      = { 
        https://doi.org/10.1086/288135
    
},
}

@Article{RSSA:RSSA12221,
  author  = {Hewson, Paul},
  title   = {Statistical Rethinking: a Bayesian Course with Examples in R and Stan R. McElreath, 2015 Boca Raton Chapman and Hall–CRC 470 pp., £60.99 ISBN 978-1-482-25344-3},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  year    = {2016},
  volume  = {179},
  number  = {4},
  pages   = {1131--1132},
  issn    = {1467-985X},
  doi     = {10.1111/rssa.12221},
  url     = {http://dx.doi.org/10.1111/rssa.12221},
}

@Article{doi:10.3102/1076998616659752,
  author  = {Tracy M. Sweet},
  title   = {A Review of Statistical Rethinking: A Bayesian Course With Examples in R and Stan},
  journal = {Journal of Educational and Behavioral Statistics},
  year    = {2017},
  volume  = {42},
  number  = {1},
  pages   = {107-110},
  doi     = {10.3102/1076998616659752},
  eprint  = {https://doi.org/10.3102/1076998616659752},
  url     = { 
        https://doi.org/10.3102/1076998616659752
    
},
}

@Article{doi:10.1177/0146167217729162,
  author   = {Andrew Gelman},
  title    = {The Failure of Null Hypothesis Significance Testing When Studying Incremental Changes, and What to Do About It},
  journal  = {Personality and Social Psychology Bulletin},
  year     = {2017},
  volume   = {0},
  number   = {0},
  pages    = {0146167217729162},
  note     = {PMID: 28914154},
  abstract = { A standard mode of inference in social and behavioral science is to establish stylized facts using statistical significance in quantitative studies. However, in a world in which measurements are noisy and effects are small, this will not work: selection on statistical significance leads to effect sizes which are overestimated and often in the wrong direction. After a brief discussion of two examples, one in economics and one in social psychology, we consider the procedural solution of open postpublication review, the design solution of devoting more effort to accurate measurements and within-person comparisons, and the statistical analysis solution of multilevel modeling and reporting all results rather than selection on significance. We argue that the current replication crisis in science arises in part from the ill effects of null hypothesis significance testing being used to study small effects with noisy data. In such settings, apparent success comes easy but truly replicable results require a more serious connection between theory, measurement, and data. },
  doi      = {10.1177/0146167217729162},
  eprint   = {https://doi.org/10.1177/0146167217729162},
  url      = { 
        https://doi.org/10.1177/0146167217729162
    
},
}

@Article{open2015estimating,
  author    = {Open Science Collaboration and others},
  title     = {Estimating the reproducibility of psychological science},
  journal   = {Science},
  year      = {2015},
  volume    = {349},
  number    = {6251},
  pages     = {aac4716},
  abstract  = {INTRODUCTION
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALE
There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTS
We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P < .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSION
No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.

Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.},
  publisher = {American Association for the Advancement of Science},
}

@Article{simmons2011false,
  author    = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
  title     = {False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant},
  journal   = {Psychological science},
  year      = {2011},
  volume    = {22},
  number    = {11},
  pages     = {1359--1366},
  abstract  = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.

Keywords methodology, motivated reasoning, publication, disclosure},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
}

@Article{doi:10.1080/01621459.2017.1289846,
  author    = {Blakeley B. McShane and David Gal},
  title     = {Statistical Significance and the Dichotomization of Evidence},
  journal   = {Journal of the American Statistical Association},
  year      = {2017},
  volume    = {112},
  number    = {519},
  pages     = {885-895},
  month     = {jul},
  abstract  = { ABSTRACTIn light of recent concerns about reproducibility and replicability, the ASA issued a Statement on Statistical Significance and p-values aimed at those who are not primarily statisticians. While the ASA Statement notes that statistical significance and p-values are “commonly misused and misinterpreted,” it does not discuss and document broader implications of these errors for the interpretation of evidence. In this article, we review research on how applied researchers who are not primarily statisticians misuse and misinterpret p-values in practice and how this can lead to errors in the interpretation of evidence. We also present new data showing, perhaps surprisingly, that researchers who are primarily statisticians are also prone to misuse and misinterpret p-values thus resulting in similar errors. In particular, we show that statisticians tend to interpret evidence dichotomously based on whether or not a p-value crosses the conventional 0.05 threshold for statistical significance. We discuss implications and offer recommendations. },
  doi       = {10.1080/01621459.2017.1289846},
  eprint    = {https://doi.org/10.1080/01621459.2017.1289846},
  publisher = {Taylor \& Francis},
  url       = { 
        https://doi.org/10.1080/01621459.2017.1289846
    
},
}

@InCollection{Philipsen2017,
  author    = {Kristian Philipsen},
  title     = {Theory Building: Using Abductive Search Strategies},
  booktitle = {Collaborative Research Design},
  publisher = {Springer Singapore},
  year      = {2017},
  pages     = {45--71},
  month     = {sep},
  abstract  = {Understanding how theory is built or comes into existence is a crucial part of knowledge production for all researchers. Knowledge production can be separated into context of discovery (problem/domain definition and initial ideas) and context of justification (hypothesis testing and hypothesis improvement). The context of justification is much better understood by researchers than is the context of discovery. The concepts of induction and deduction are used in the context of justification but they cannot explain how new ideas are created. This chapter proposes the concept of abduction to address this. Abduction can be conceptualized as making guesses. In a theory building process there is a need to make guesses when a researcher makes observations which are surprising in that they depart from existing theory. Other researchers have used abduction to describe and understand theory building. This paper identifies anomalies in the existing frameworks and proposes further development of the framework of systematic combining of the theory and empirical world. This chapter argues that the concept of abduction can contribute to understand how theory is created in theory testing, theory development and theory creation, but focuses on theory building in both realist and an interpretive research.},
  doi       = {10.1007/978-981-10-5008-4_3},
}

@Article{Gelman2014,
  author    = {Andrew Gelman and John Carlin},
  title     = {Beyond Power Calculations},
  journal   = {Perspectives on Psychological Science},
  year      = {2014},
  volume    = {9},
  number    = {6},
  pages     = {641--651},
  month     = {nov},
  abstract  = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  doi       = {10.1177/1745691614551642},
  publisher = {{SAGE} Publications},
}

@Article{Wicherts2017,
  author    = {Jelte Wicherts},
  title     = {The Weak Spots in Contemporary Science (and How to Fix Them)},
  journal   = {Animals},
  year      = {2017},
  volume    = {7},
  number    = {12},
  pages     = {90},
  month     = {nov},
  abstract  = {Simple Summary: Several fraud cases, widespread failure to replicate or reproduce seminal findings, and pervasive error in the scientific literature have led to a crisis of confidence in the biomedical, behavioral, and social sciences. In this review, the author discusses some of the core findings that point at weak spots in contemporary science and considers the human factors that underlie them. He delves into the human tendencies that create errors and biases in data collection, analyses, and reporting of research results. He presents several solutions to deal with observer bias, publication bias, the researcher’s tendency to exploit degrees of freedom in their analysis of data, low statistical power, and errors in the reporting of results, with a focus on the specific challenges in animal welfare research.
Abstract: In this review, the author discusses several of the weak spots in contemporary science, including scientific misconduct, the problems of post hoc hypothesizing (HARKing), outcome switching, theoretical bloopers in formulating research questions and hypotheses, selective reading of the literature, selective citing of previous results, improper blinding and other design failures, p-hacking or researchers’ tendency to analyze data in many different ways to find positive (typically significant) results, errors and biases in the reporting of results, and publication bias. The author presents some empirical results highlighting problems that lower the trustworthiness of reported results in scientific literatures, including that of animal welfare studies. Some of the underlying causes of these biases are discussed based on the notion that researchers are only human and hence are not immune to confirmation bias, hindsight bias, and minor ethical transgressions. The author discusses solutions in the form of enhanced transparency, sharing of data and materials, (post-publication) peer review, pre-registration, registered reports, improved training, reporting guidelines, replication, dealing with publication bias, alternative inferential techniques, power, and other statistical tools.
Keywords: reproducibility; replicability; validity; questionable research practices; meta-research},
  doi       = {10.3390/ani7120090},
  publisher = {{MDPI} {AG}},
}

@Article{Carpenter2017,
  author    = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  title     = {Stan: A Probabilistic Programming Language},
  journal   = {Journal of Statistical Software},
  year      = {2017},
  volume    = {76},
  number    = {1},
  doi       = {10.18637/jss.v076.i01},
  publisher = {Foundation for Open Access Statistic},
}

@Article{Song2012,
  author    = {Xin-Yuan Song and Sik-Yum Lee},
  title     = {A tutorial on the Bayesian approach for analyzing structural equation models},
  journal   = {Journal of Mathematical Psychology},
  year      = {2012},
  volume    = {56},
  number    = {3},
  pages     = {135--148},
  month     = {jun},
  abstract  = {In this paper, we provide a tutorial exposition on the Bayesian approach in analyzing structural equation models (SEMs). SEMs, which can be regarded as regression models with observed and latent variables, have been widely applied to substantive research. However, the classical methods and most commercial software in this area are based on the covariance structure approach, which would encounter serious difficulties when dealing with complicated models and/or data structures. In contrast, the Bayesian approach has much more flexibility in handling complex situations. We give a brief introduction to SEMs and a detailed description of how to apply the Bayesian approach to this kind of model. Advantages of the Bayesian approach are discussed, and results obtained from a simulation study are provided for illustration. The intended audience is statisticians/methodologists who either know about SEMs or simple Bayesian statistics, and Ph.D. students in statistics, psychometrics, or mathematical psychology.},
  doi       = {10.1016/j.jmp.2012.02.001},
  publisher = {Elsevier {BV}},
}

@Comment{jabref-meta: databaseType:bibtex;}
