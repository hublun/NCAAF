% Encoding: UTF-8

@Article{2017arXiv171106402A,
  author        = {{Avati}, A. and {Jung}, K. and {Harman}, S. and {Downing}, L. and {Ng}, A. and {Shah}, N.~H.},
  title         = {{Improving Palliative Care with Deep Learning}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {Improving the quality of end-of-life care for hospitalized patients is a priority for healthcare organizations. Studies have shown that physicians tend to over-estimate prognoses, which in combination with treatment inertia results in a mismatch between patients wishes and actual care at the end of life. We describe a method to address this problem using Deep Learning and Electronic Health Record (EHR) data, which is currently being piloted, with Institutional Review Board approval, at an academic medical center. The EHR data of admitted patients are automatically evaluated by an algorithm, which brings patients who are likely to benefit from palliative care services to the attention of the Palliative Care team. The algorithm is a Deep Neural Network trained on the EHR data from previous years, to predict all-cause 3-12 month mortality of patients as a proxy for patients that could benefit from palliative care. Our predictions enable the Palliative Care team to take a proactive approach in reaching out to such patients, rather than relying on referrals from treating physicians, or conduct time consuming chart reviews of all patients. We also present a novel interpretation technique which we use to provide explanations of the model's predictions.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171106402A},
  archiveprefix = {arXiv},
  eprint        = {1711.06402},
  keywords      = {Computer Science - Computers and Society, Computer Science - Learning, Statistics - Machine Learning},
  primaryclass  = {cs.CY},
}

@Article{2017arXiv171105225R,
  author        = {{Rajpurkar}, P. and {Irvin}, J. and {Zhu}, K. and {Yang}, B. and {Mehta}, H. and {Duan}, T. and {Ding}, D. and {Bagul}, A. and {Langlotz}, C. and {Shpanskaya}, K. and {Lungren}, M.~P. and {Ng}, A.~Y.},
  title         = {{CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on pneumonia detection on both sensitivity and specificity. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171105225R},
  archiveprefix = {arXiv},
  eprint        = {1711.05225},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
  primaryclass  = {cs.CV},
}

@Article{2017arXiv171110337L,
  author        = {{Lucic}, M. and {Kurach}, K. and {Michalski}, M. and {Gelly}, S. and {Bousquet}, O.},
  title         = {{Are GANs Created Equal? A Large-Scale Study}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = nov,
  abstract      = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the original one.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv171110337L},
  archiveprefix = {arXiv},
  eprint        = {1711.10337},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning},
  primaryclass  = {stat.ML},
}

@Article{Poplin2018,
  author    = {Ryan Poplin and Avinash V. Varadarajan and Katy Blumer and Yun Liu and Michael V. McConnell and Greg S. Corrado and Lily Peng and Dale R. Webster},
  title     = {Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning},
  journal   = {Nature Biomedical Engineering},
  year      = {2018},
  month     = {feb},
  abstract  = {Traditionally, medical discoveries are made by observing associations, making hypotheses from them and then designing and running experiments to test the hypotheses. However, with medical images, observing and quantifying associations can often be difficult because of the wide variety of features, patterns, colours, values and shapes that are present in real data. Here, we show that deep learning can extract new knowledge from retinal fundus images. Using deep-learning models trained on data from 284,335 patients and validated on two independent datasets of 12,026 and 999 patients, we predicted cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as age (mean absolute error within 3.26 years), gender (area under the receiver operating characteristic curve (AUC) = 0.97), smoking status (AUC = 0.71), systolic blood pressure (mean absolute error within 11.23 mmHg) and major adverse cardiac events (AUC = 0.70). We also show that the trained deep-learning models used anatomical features, such as the optic disc or blood vessels, to generate each prediction.

},
  doi       = {10.1038/s41551-018-0195-0},
  file      = {:D\:/Github/Informs/Articles/41551_2018_195_MOESM1_ESM.pdf:PDF;:D\:/Github/Informs/Articles/Prediction of cardiovascular risk factors from.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Brown2017,
  author    = {Noam Brown and Tuomas Sandholm},
  title     = {Superhuman {AI} for heads-up no-limit poker: Libratus beats top professionals},
  journal   = {Science},
  year      = {2017},
  volume    = {359},
  number    = {6374},
  pages     = {418--424},
  month     = {dec},
  doi       = {10.1126/science.aao1733},
  file      = {:D\:/Dropbox/AI Impact/science.aao1733.full.pdf:PDF},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{shallue2018identifying,
  author    = {Shallue, Christopher J and Vanderburg, Andrew},
  title     = {Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90},
  journal   = {The Astronomical Journal},
  year      = {2018},
  volume    = {155},
  number    = {2},
  pages     = {94},
  file      = {:C\:/Users/cjdua/Documents/kepler90i.pdf:PDF},
  publisher = {IOP Publishing},
}

@Article{Chen2017,
  author    = {Gang Chen and Yaqiong Xiao and Paul A Taylor and Justin K Rajendra and Tracy Riggins and Fengji Geng and Elizabeth Redcay and Robert W Cox},
  title     = {Handling Multiplicity in Neuroimaging through Bayesian Lenses with Multilevel Modeling},
  year      = {2017},
  month     = {dec},
  doi       = {10.1101/238998},
  file      = {:C\:/Users/cjdua/Documents/238998.full.pdf:PDF},
  publisher = {Cold Spring Harbor Laboratory},
}

@Article{marcus2018deep,
  author  = {Marcus, Gary},
  title   = {Deep Learning: A Critical Appraisal},
  journal = {arXiv preprint arXiv:1801.00631},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1801.00631.pdf:PDF},
}

@Article{Boullier2016,
  author  = {Boullier, Dominique},
  title   = {Big data challenges for the social sciences: From society and opinion to replications},
  journal = {arXiv preprint arXiv:1607.05034},
  year    = {2016},
  file    = {:C\:/Users/cjdua/Documents/EBul-Boullier-Jul2017.pdf:PDF},
}

@Article{Jia2017,
  author  = {Jia, Jie and Zhou, Honggang and Li, Yunchun},
  title   = {Using Deep Neural Network Approximate Bayesian Network},
  journal = {arXiv preprint arXiv:1801.00282},
  year    = {2017},
  file    = {:C\:/Users/cjdua/Documents/1801.00282.pdf:PDF},
}

@Article{pearl2018theoretical,
  author  = {Pearl, Judea},
  title   = {Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution},
  journal = {arXiv preprint arXiv:1801.04016},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1801.04016.pdf:PDF},
}

@InCollection{taddy2018technological,
  author    = {Taddy, Matt},
  title     = {The Technological Elements of Artificial Intelligence},
  booktitle = {Economics of Artificial Intelligence},
  publisher = {University of Chicago Press},
  year      = {2018},
  file      = {:C\:/Users/cjdua/Documents/c14021.pdf:PDF},
}

@Article{sargsyan2018embedded,
  author  = {Sargsyan, Khachik and Huan, Xun and Najm, Habib N},
  title   = {Embedded Model Error Representation for Bayesian Model Calibration},
  journal = {arXiv preprint arXiv:1801.06768},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1801.06768.pdf:PDF},
}

@Article{rajkomar2018scalable,
  author  = {Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M and Hajaj, Nissan and Liu, Peter J and Liu, Xiaobing and Sun, Mimi and Sundberg, Patrik and Yee, Hector and others},
  title   = {Scalable and accurate deep learning for electronic health records},
  journal = {arXiv preprint arXiv:1801.07860},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1801.07860.pdf:PDF},
}

@Article{williams2006gaussian,
  author  = {Williams, Christopher KI and Rasmussen, Carl Edward},
  title   = {Gaussian processes for machine learning},
  journal = {the MIT Press},
  year    = {2006},
  volume  = {2},
  number  = {3},
  pages   = {4},
  file    = {:C\:/Users/cjdua/Documents/RW.pdf:PDF},
}

@Article{Caliskan2017,
  author    = {Aylin Caliskan and Joanna J. Bryson and Arvind Narayanan},
  title     = {Semantics derived automatically from language corpora contain human-like biases},
  journal   = {Science},
  year      = {2017},
  volume    = {356},
  number    = {6334},
  pages     = {183--186},
  month     = {apr},
  abstract  = {Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicate a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
  doi       = {10.1126/science.aal4230},
  file      = {:C\:/Users/cjdua/Documents/155388415.pdf:PDF},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Theis2015,
  author      = {Lucas Theis and Aäron van den Oord and Matthias Bethge},
  title       = {A note on the evaluation of generative models},
  abstract    = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
  date        = {2015-11-05},
  eprint      = {1511.01844v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  keywords    = {stat.ML, cs.LG},
}

@Article{Lucic2017,
  author      = {Mario Lucic and Karol Kurach and Marcin Michalski and Sylvain Gelly and Olivier Bousquet},
  title       = {Are GANs Created Equal? A Large-Scale Study},
  abstract    = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the original one.},
  date        = {2017-11-28},
  eprint      = {1711.10337v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1711.10337v3:PDF;:D\:/Github/Informs/Articles/2018.arXiv.1711.10337.pdf:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Koh2017,
  author      = {Pang Wei Koh and Percy Liang},
  title       = {Understanding Black-box Predictions via Influence Functions},
  abstract    = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  date        = {2017-03-14},
  eprint      = {1703.04730v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:D\:/Github/Informs/Articles/2017.arXiv.1703.04730.pdf:URL},
  keywords    = {stat.ML, cs.AI, cs.LG},
}

@Article{Ching20170387,
  author    = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M. and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A. and Turaga, Srinivas C. and Alexandari, Amr M. and Lu, Zhiyong and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Boca, Simina M. and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
  title     = {Opportunities and obstacles for deep learning in biology and medicine},
  journal   = {Journal of The Royal Society Interface},
  year      = {2018},
  volume    = {15},
  number    = {141},
  issn      = {1742-5689},
  abstract  = {Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems{\textemdash}patient classification, fundamental biological processes and treatment of patients{\textemdash}and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network{\textquoteright}s prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.},
  doi       = {10.1098/rsif.2017.0387},
  eprint    = {http://rsif.royalsocietypublishing.org/content/15/141/20170387.full.pdf},
  file      = {:D\:/Github/Informs/Articles/20170387.full.pdf:PDF},
  publisher = {The Royal Society},
  url       = {http://rsif.royalsocietypublishing.org/content/15/141/20170387},
}

@Article{Yeung2018,
  author    = {Serena Yeung and N. Lance Downing and Li Fei-Fei and Arnold Milstein},
  title     = {Bedside Computer Vision {\textemdash} Moving Artificial Intelligence from Driver Assistance to Patient Safety},
  journal   = {New England Journal of Medicine},
  year      = {2018},
  volume    = {378},
  number    = {14},
  pages     = {1271--1273},
  month     = {apr},
  doi       = {10.1056/nejmp1716891},
  file      = {:D\:/Github/Informs/Articles/nejmp1716891[1819].pdf:PDF},
  publisher = {New England Journal of Medicine ({NEJM}/{MMS})},
}

@Article{Caselli2018,
  author   = {Francesco Caselli and Alan Manning},
  title    = {Robot Arithmetic: New Technology And Wages},
  journal  = {AER Insights},
  year     = {2018},
  abstract = {Existing economic models show how new technology can cause large changes in relative
wages and inequality. But there are also claims, based largely on verbal expositions, that new
technology can harm workers on average or even all workers. This paper shows – under
plausible assumptions - that new technology is unlikely to cause wages for all workers to fall
and will cause average wages to rise if the prices of investment goods fall relative to
consumer goods (a condition supported by the data). We outline how results may change
with different assumptions.},
  doi      = {10.1257/aeri.20170036},
  file     = {:D\:/Github/Informs/Articles/Manning__robot-arithmatic--author-merged.pdf:PDF},
}

@Article{Ha2018,
  author      = {David Ha and Jürgen Schmidhuber},
  title       = {World Models},
  abstract    = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  date        = {2018-03-27},
  doi         = {10.5281/zenodo.1207631},
  eprint      = {1803.10122v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:D\:/Github/Informs/Articles/1803.10122.pdf:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Kaunitz2017,
  author        = {Lisandro Kaunitz and Shenjun Zhong and Javier Kreiner},
  title         = {Beating the bookies with their own numbers - and how the online sports betting market is rigged},
  __markedentry = {[dulunche:]},
  abstract      = {The online sports gambling industry employs teams of data analysts to build forecast models that turn the odds at sports games in their favour. While several betting strategies have been proposed to beat bookmakers, from expert prediction models and arbitrage strategies to odds bias exploitation, their returns have been inconsistent and it remains to be shown that a betting strategy can outperform the online sports betting market. We designed a strategy to beat football bookmakers with their own numbers. Instead of building a forecasting model to compete with bookmakers predictions, we exploited the probability information implicit in the odds publicly available in the marketplace to find bets with mispriced odds. Our strategy proved profitable in a 10-year historical simulation using closing odds, a 6-month historical simulation using minute to minute odds, and a 5-month period during which we staked real money with the bookmakers (we made code, data and models publicly available). Our results demonstrate that the football betting market is inefficient - bookmakers can be consistently beaten across thousands of games in both simulated environments and real-life betting. We provide a detailed description of our betting experience to illustrate how the sports gambling industry compensates these market inefficiencies with discriminatory practices against successful clients.},
  date          = {2017-10-08},
  eprint        = {1710.02824v2},
  eprintclass   = {stat.AP},
  eprinttype    = {arXiv},
  file          = {:D\:/Github/Informs/Articles/1710.02824.pdf:PDF},
  keywords      = {stat.AP, cs.OH, stat.OT},
}

@Article{Oliver2018,
  author        = {Avital Oliver and Augustus Odena and Colin Raffel and Ekin D. Cubuk and Ian J. Goodfellow},
  title         = {Realistic Evaluation of Deep Semi-Supervised Learning Algorithms},
  __markedentry = {[dulunche:6]},
  abstract      = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
  date          = {2018-04-24},
  eprint        = {1804.09170v1},
  eprintclass   = {cs.LG},
  eprinttype    = {arXiv},
  file          = {:D\:/Github/Informs/Articles/1804.09170.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
}

@Article{Brynjolfsson2017,
  author    = {Erik Brynjolfsson and Tom Mitchell},
  title     = {What can machine learning do? Workforce implications},
  journal   = {Science},
  year      = {2017},
  volume    = {358},
  number    = {6370},
  pages     = {1530--1534},
  month     = {dec},
  doi       = {10.1126/science.aap8062},
  file      = {:D\:/Github/Informs/Articles/Science_WorkforceDec2017.pdf:PDF},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Comment{jabref-meta: databaseType:bibtex;}
